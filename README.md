# ğŸ—ï¸ RSNA Breast Cancer Detection

## ğŸ“‹ Overview

This project provides innovative solutions for the [RSNA Screening Mammography Breast Cancer Detection competition](https://www.kaggle.com/competitions/rsna-breast-cancer-detection) organized by the Radiological Society of North America (RSNA) in partnership with Kaggle.

Breast cancer is the most common cancer worldwide. In 2020, there were 2.3 million new diagnoses and 685,000 deaths. Our solutions aim to develop AI systems capable of assisting radiologists in early breast cancer detection on screening mammograms.

### ğŸ‘¥ Team Collaboration

This is a **collaborative project** with four different model architectures, each developed by a team member:

1. **DIABIRA** - Multi-Head Ensemble Architecture
2. **ABBASI** - Advanced Deep Learning Approach
3. **MANOUR** - Specialized CNN Model
4. **BENHAMOUCHE** - Hybrid Architecture


## ğŸ—‚ï¸ Project Structure

```
rsna-breast-cancer-detection/
â”œâ”€â”€ README.md
â”œâ”€â”€ pixi.toml                      # Pixi configuration
â”œâ”€â”€ .gitignore
â”‚â”€â”€ EDA.ipynb                      # Exploratory Data Analysis
â”‚
â”œâ”€â”€ core/                          # Core utilities and shared modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”œâ”€â”€ data_loader.py            # Data loading utilities
â”‚   â””â”€â”€ utils.py                  # General utilities
â”‚
â”œâ”€â”€ preprocess/                    # ğŸ“ Preprocessing modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dicom_processor.py        # DICOM loading and processing
â”‚   â”œâ”€â”€ normalization.py          # Intensity normalization
â”‚   â”œâ”€â”€ augmentation.py           # Data augmentation
â”‚   â”œâ”€â”€ cleaning.py               # Data cleaning strategies
â”‚   â””â”€â”€ splitter.py               # Train/val/test splitting
â”‚
â”œâ”€â”€ models/                        # ğŸ§  Model architectures (4 approaches)
â”‚   â”‚
â”‚   â”œâ”€â”€ DIABIRA/                  # 1ï¸âƒ£ Primary Model - Multi-Head Ensemble
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ multi_head_model.py   # Main architecture
â”‚   â”‚   â”œâ”€â”€ heads/                # Specialized heads
â”‚   â”‚   â”‚   â”œâ”€â”€ efficient_net.py
â”‚   â”‚   â”‚   â”œâ”€â”€ convnext.py
â”‚   â”‚   â”‚   â””â”€â”€ swin_transformer.py
â”‚   â”‚   â”œâ”€â”€ fusion.py             # Fusion mechanism
â”‚   â”‚   â”œâ”€â”€ losses.py             # Adaptive Focal Loss
â”‚   â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”‚   â””â”€â”€ README.md             # Model-specific documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ ABBASI/                   # 2ï¸âƒ£ Second Model
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ MANOUR/                   # 3ï¸âƒ£ Third Model
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â””â”€â”€ BENHAMOUCHE/              # 4ï¸âƒ£ Fourth Model
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ model.py
â”‚       â”œâ”€â”€ train.py
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/                          # Data (not versioned)
â”‚   â”œâ”€â”€ raw/                      # Original DICOM files
â”‚   â”‚   â”œâ”€â”€ train_images/
â”‚   â”‚   â””â”€â”€ test_images/
â”‚   â”œâ”€â”€ processed/                # Preprocessed data
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â””â”€â”€ eda/                      # EDA outputs
â”‚
â”œâ”€â”€ scripts/                       # Utility scripts
â”‚   â”œâ”€â”€ download_data.py          # Download from Kaggle
â”‚   â”œâ”€â”€ run_preprocessing.py      # Run preprocessing pipeline
â”‚   â”œâ”€â”€ compare_models.py         # Compare all 4 models
â”‚   â””â”€â”€ create_submission.py      # Create submission file
â”‚
â”œâ”€â”€ checkpoints/                   # Model checkpoints (not versioned)
â”‚   â”œâ”€â”€ DIABIRA/
â”‚   â”œâ”€â”€ ABBASI/
â”‚   â”œâ”€â”€ MANOUR/
â”‚   â””â”€â”€ BENHAMOUCHE/
â”‚
â”œâ”€â”€ runs/                          # TensorBoard logs (not versioned)
â”œâ”€â”€ results/                       # Results and analysis
â””â”€â”€ submissions/                   # Submission files
```

## ğŸ“Š Exploratory Data Analysis (EDA)

Before building models, we conducted comprehensive exploratory data analysis documented in `notebooks/EDA.ipynb`:


### EDA Notebook Contents

The `notebooks/EDA.ipynb` includes:
1. **Data Loading & Validation**: CSV and DICOM file structure
2. **Statistical Analysis**: Distributions, correlations, chi-square tests
3. **Visualization**: Class balance, age distribution, site comparison
4. **Image Analysis**: Sample images, intensity histograms, view types
5. **Consistency Checks**: CSV-to-image mapping verification
6. **Insights & Recommendations**: Preprocessing strategy, model requirements

## ğŸ”§ Core Modules

The `core/` directory contains shared utilities used across all models:

### config.py
Configuration management for experiments:
```python
from core.config import Config

config = Config()
config.batch_size = 16
config.learning_rate = 1e-4
config.device = 'auto'  # Auto-detect GPU/CPU
```

### data_loader.py
Unified data loading interface:
```python
from core.data_loader import get_dataloaders

train_loader, val_loader, test_loader = get_dataloaders(
    data_dir='data/processed',
    batch_size=16,
    num_workers=8
)
```

### metrics.py
Comprehensive evaluation metrics:
```python
from core.metrics import calculate_metrics

metrics = calculate_metrics(y_true, y_pred, y_proba)
# Returns: f1, precision, recall, auc_roc, auc_pr, specificity
```

### visualization.py
Plotting and visualization tools:
```python
from core.visualization import plot_training_curves, plot_confusion_matrix

plot_training_curves(history, save_path='results/training.png')
plot_confusion_matrix(y_true, y_pred, save_path='results/cm.png')
```

## ğŸ“ Preprocessing Pipeline

The `preprocess/` directory contains modular preprocessing components:

### 1. DICOM Processing (`dicom_processor.py`)

Load and decode DICOM medical images:
```python
from preprocess.dicom_processor import DICOMProcessor

processor = DICOMProcessor()
pixel_array = processor.load_dicom(dicom_path)
# Handles: MONOCHROME1/2, decompression, metadata extraction
```

**Features:**
- âœ… Automatic MONOCHROME1 inversion
- âœ… DICOM metadata extraction
- âœ… Support for compressed formats
- âœ… Batch processing capabilities

### 2. Normalization (`normalization.py`)

Intensity normalization for mammography:
```python
from preprocess.normalization import normalize_mammogram

normalized = normalize_mammogram(
    image,
    window_center='auto',  # Auto-detect optimal window
    window_width='auto',
    target_range=(0, 255)
)
```

**Techniques:**
- ğŸšï¸ Windowing/Level adjustment
- ğŸ“Š Histogram equalization
- ğŸ”¢ Z-score normalization
- ğŸ¯ Percentile-based clipping

### 3. Data Augmentation (`augmentation.py`)

Adaptive augmentation based on class:
```python
from preprocess.augmentation import get_augmentation_pipeline

# Different augmentation for cancer vs non-cancer
aug_positive = get_augmentation_pipeline(is_cancer=True, intensity='high')
aug_negative = get_augmentation_pipeline(is_cancer=False, intensity='low')
```

**Transformations:**
- ğŸ”„ Rotation (Â±15Â°)
- â†”ï¸ Horizontal/Vertical flips
- ğŸ¨ Contrast adjustment
- ğŸ” Random zoom
- ğŸŒŠ Elastic deformation (for cancer cases)

### 4. Data Cleaning (`cleaning.py`)

Strategic undersampling to handle imbalance:
```python
from preprocess.cleaning import clean_dataset

df_cleaned = clean_dataset(
    df_original,
    neg_fraction=0.25,  # Keep 25% of negative patients
    keep_all_cancer=True,  # Never remove cancer cases
    random_seed=42
)
```

**Strategy:**
- âœ… Keep **ALL** cancer patients (483 patients)
- ğŸ“‰ Sample **25%** of non-cancer patients
- ğŸ¯ Result: 2.12% â†’ 7.32% cancer rate
- ğŸ’¾ Reduce dataset from 293GB â†’ 83GB

### 5. Train/Val/Test Splitting (`splitter.py`)

Patient-level stratified splitting to prevent data leakage:
```python
from preprocess.splitter import split_dataset

train_df, val_df, test_df = split_dataset(
    df_cleaned,
    train_size=0.70,
    val_size=0.15,
    test_size=0.15,
    stratify_by='patient_id',  # CRITICAL: No patient in multiple sets
    random_seed=42
)
```

**Key Features:**
- ğŸ‘¤ **Patient-level split** (prevents data leakage)
- âš–ï¸ Stratified by cancer status
- ğŸ“Š Maintains class balance across splits
- ğŸ”’ Reproducible with fixed random seed

### Preprocessing Statistics

| Stage          | Patients | Cancer Patients | Images | Cancer Images | Size (GB) | Cancer Rate |
|----------------|----------|-----------------|--------|---------------|-----------|-------------|
| **Initial**    | 11,913   | 486 (4.08%)    | 54,706 | 1,158 (2.12%) | 293.10    | 2.12%       |
| **Cleaned**    | 3,344    | 483 (14.44%)   | 13,379 | 979 (7.32%)   | 82.87     | 7.32%       |
| **Train**      | 2,340    | 338 (14.44%)   | 9,363  | 685 (7.32%)   | 57.99     | 7.32%       |
| **Validation** | 502      | 73 (14.54%)    | 2,008  | 150 (7.47%)   | 12.35     | 7.47%       |
| **Test**       | 502      | 72 (14.34%)    | 2,008  | 144 (7.17%)   | 12.52     | 7.17%       |

## ğŸ§  Model 1: DIABIRA - Multi-Head Ensemble Architecture

Our primary model uses a multi-head ensemble approach with specialized experts.

### Architecture Overview

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   DICOM Image (1 channel)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    DICOM Preprocessing      â”‚
                    â”‚  (from preprocess/ module)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚                         â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚  Head 1 â”‚             â”‚   Head 2   â”‚          â”‚   Head 3    â”‚
   â”‚EfficientNetâ”‚          â”‚EfficientNetâ”‚          â”‚  ConvNeXt   â”‚
   â”‚   V2-S  â”‚             â”‚    B1      â”‚          â”‚   Small     â”‚
   â”‚(Detection)â”‚           â”‚ (Texture)  â”‚          â”‚ (Context)   â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚                         â”‚                         â”‚
        â”‚                    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                    â”‚
        â”‚                    â”‚  Head 4 â”‚                    â”‚
        â”‚                    â”‚  Swin   â”‚                    â”‚
        â”‚                    â”‚Transformâ”‚                    â”‚
        â”‚                    â”‚ (Global)â”‚                    â”‚
        â”‚                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                    â”‚
        â”‚                         â”‚                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Fusion Module              â”‚
                    â”‚  â€¢ Multi-head Attention     â”‚
                    â”‚  â€¢ Adaptive Weighting       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Final Prediction           â”‚
                    â”‚  (Cancer Probability)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The 4 Specialized Heads

1. **RSNA Detector Head** (`heads/efficient_net.py`)
   - Backbone: Pre-trained EfficientNetV2-S
   - Specialization: Localized lesion detection
   - Focus: Microcalcifications and suspicious masses

2. **Kaggle Texture Head** (`heads/efficient_net.py`)
   - Backbone: EfficientNet-B1
   - Specialization: Textural property analysis
   - Focus: Homogeneity, granularity, local contrasts

3. **ConvNeXt Context Head** (`heads/convnext.py`)
   - Backbone: ConvNeXt-Small
   - Specialization: Large-scale contextual information
   - Focus: Spatial distribution, density, symmetry

4. **Swin Global Vision Head** (`heads/swin_transformer.py`)
   - Backbone: Swin Transformer
   - Specialization: Hierarchical global understanding
   - Focus: Long-range relationships, overall view

### Key Features

- **Adaptive Focal Loss**: Handles class imbalance
- **Multi-head Attention**: Dynamic expert weighting
- **Progressive Training**: Warmup â†’ Fine-tuning â†’ Refinement
- **GPU Optimization**: Automatic resource detection

For detailed information, see [`models/DIABIRA/README.md`](models/DIABIRA/README.md)

## ğŸ¤– Other Models

### Model 2: ABBASI
See [`models/ABBASI/README.md`](models/ABBASI/README.md) for architecture details and results.

### Model 3: MANOUR
See [`models/MANOUR/README.md`](models/MANOUR/README.md) for architecture details and results.

### Model 4: BENHAMOUCHE
See [`models/BENHAMOUCHE/README.md`](models/BENHAMOUCHE/README.md) for architecture details and results.

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (for NVIDIA GPU) or Metal (for Mac M-series)
- 16GB+ RAM
- 50GB+ disk space
- Pixi package manager

### Installation with Pixi (Recommended)

#### Install Pixi

**macOS/Linux:**
```bash
curl -fsSL https://pixi.sh/install.sh | bash
```

**Windows:**
```powershell
iwr -useb https://pixi.sh/install.ps1 | iex
```

#### Setup Project

```bash
# Clone the repository
git clone https://github.com/your-username/rsna-breast-cancer-detection.git
cd rsna-breast-cancer-detection

# Install dependencies with Pixi
pixi install

# Activate the environment
pixi shell

# Verify installation
python --version
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
```

## ğŸ® Usage

### 1. Explore the Data (EDA)

```bash
# Launch Jupyter notebook
pixi run notebook

# Open notebooks/EDA.ipynb
# Explore data distributions, visualizations, and insights
```

### 2. Preprocess the Data

```bash
# Run full preprocessing pipeline
pixi run preprocess-clean

# Or run step-by-step
python -m preprocess.dicom_processor --input data/raw/train_images
python -m preprocess.cleaning --neg_fraction 0.25
python -m preprocess.splitter --train_size 0.70
```

### 3. Train Models

#### Train DIABIRA Model (Primary)
```bash
# Using Pixi task
pixi run train-diabira

# Or manually
cd models/DIABIRA
python train.py --data_dir ../../data/processed --epochs 30
```

#### Train Other Models
```bash
# ABBASI model
cd models/ABBASI
python train.py

# MANOUR model
cd models/MANOUR
python train.py

# BENHAMOUCHE model
cd models/BENHAMOUCHE
python train.py
```

### 4. Compare Models

```bash
# Run model comparison script
python scripts/compare_models.py \
    --models DIABIRA ABBASI MANOUR BENHAMOUCHE \
    --metrics f1 auc_roc auc_pr \
    --output results/comparison.csv
```

### 5. Create Submission

```bash
# Generate predictions from best model
python scripts/create_submission.py \
    --model DIABIRA \
    --checkpoint checkpoints/DIABIRA/best_model.pth \
    --output submissions/submission.csv
```

## ğŸ“ˆ Results and Metrics

### Evaluation Metrics
- **F1-Score**: Balance between precision and recall
- **AUC-ROC**: Overall classification performance
- **AUC-PR**: Particularly suited for imbalanced classes
- **Sensitivity** (Recall): True positive detection rate
- **Specificity**: True negative identification rate

### Model Comparison (Example)

| Model       | F1-Score | AUC-ROC | AUC-PR | Sensitivity | Specificity | Training Time |
|-------------|----------|---------|--------|-------------|-------------|---------------|
| DIABIRA     | 0.512    | 0.891   | 0.389  | 0.654       | 0.921       | 8h 23m        |
| ABBASI      | TBD      | TBD     | TBD    | TBD         | TBD         | TBD           |
| MANOUR      | TBD      | TBD     | TBD    | TBD         | TBD         | TBD           |
| BENHAMOUCHE | TBD      | TBD     | TBD    | TBD         | TBD         | TBD           |

*Note: Fill in results as models are trained*

## ğŸ² Strategies for Imbalanced Classes

All models implement strategies to handle the severe class imbalance:

### 1. Adaptive Focal Loss
```python
# Penalizes errors on minority class more heavily
Loss = -Î±(1-p)^Î³ log(p)
```

### 2. Adaptive Data Augmentation
- **For positive cases (cancer)**: Aggressive augmentation
- **For negative cases**: Minimal augmentation

### 3. Stratified Resampling
- Oversampling positive cases
- Light undersampling of negative cases
- Patient-stratified validation

### 4. Optimized Decision Threshold
- Threshold calibration to maximize F1-score
- Precision-Recall curve optimization

## âš™ï¸ GPU/CPU Optimizations

Training pipeline automatically detects and optimizes resource utilization:

### Automatic Detection
- Device (MPS for Mac M-series, CUDA for NVIDIA, CPU otherwise)
- Number of available GPUs
- GPU and CPU RAM
- Optimal number of workers
- Optimal batch size

### Implemented Optimizations
- âœ… Multi-GPU Parallelization
- âœ… Asynchronous Data Loading
- âœ… Pin Memory (CUDA)
- âœ… Gradient Accumulation
- âœ… Mixed Precision Training
- âœ… Validation Caching
- âœ… Gradient Checkpointing

## ğŸ¤ Contributing

This is a collaborative project. Each team member maintains their model in a separate directory.

### Adding a New Model

1. Create directory: `models/YOUR_NAME/`
2. Implement model architecture
3. Create training script
4. Add README with architecture details
5. Update main README with model description

### Code Guidelines
- Documented and commented code
- Follow PEP 8 conventions
- Use shared utilities from `core/` and `preprocess/`
- Update model comparison table with results

## ğŸ‘¥ Team Members

- **Assa DIABIRA** - Multi-Head Ensemble Architecture
- **ABBASI** - Advanced Deep Learning Approach
- **MANOUR** - Specialized CNN Model  
- **BENHAMOUCHE** - Hybrid Architecture

## ğŸ“ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **RSNA** for organizing the competition and providing the dataset
- **Kaggle** for the competition platform
- Mammography screening programs in Australia and the US for the data
- PyTorch and timm communities for model implementations

## ğŸ“š References

### Scientific Papers
- [Deep Learning for Breast Cancer Detection](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10077079/)
- [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)
- [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298)
- [Swin Transformer: Hierarchical Vision Transformer](https://arxiv.org/abs/2103.14030)

### Competition and Datasets
- [RSNA Screening Mammography Breast Cancer Detection](https://www.kaggle.com/competitions/rsna-breast-cancer-detection)
- [RSNA Official Challenge Page](https://www.rsna.org/rsnai/ai-image-challenge/screening-mammography-breast-cancer-detection-ai-challenge)
- [1st Place Solution](https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/392449)

## ğŸ“§ Contact

For questions or suggestions, feel free to open an issue or contact team members.

---

â­ If this project was useful to you, don't forget to give it a star!

ğŸ—ï¸ **Together against breast cancer** - Every early detection counts.